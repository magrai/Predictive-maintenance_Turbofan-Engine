---
title: "Turbofan Engine Degradation Simulation Data Set"
output:
  html_document: default
  html_notebook: default
---

# Description

Engine degradation simulation was carried out using C-MAPSS. Four different were sets simulated under different combinations of operational conditions and fault modes. Records several sensor channels to characterize fault evolution. The data set was provided by the Prognostics CoE at NASA Ames.

Reference: A. Saxena, K. Goebel, D. Simon, and N. Eklund, “Damage Propagation Modeling for Aircraft Engine Run-to-Failure Simulation”, in the Proceedings of the Ist International Conference on Prognostics and Health Management (PHM08), Denver CO, Oct 2008.

https://ti.arc.nasa.gov/tech/dash/pcoe/prognostic-data-repository/

***

We've acquired a fleet of 100 aircraft engines. The manufacturer recommends that we perform maintenance after every 125 flights. After our first round of maintenance, our maintenance workers tell us that only engines 39, 57, 70, and 91 were badly in need of maintenance. The others seemed fine and could have run longer without servicing. This means we are wasting money by doing maintenance more often than needed. We'd like to develop a better way to identify when servicing is needed so we can be smarter about scheduling our maintenance.

# Read in first engine data

The data used is the sensor readings taken off of the equipment. Maintenance was done after 125 flights, regardless of wheather the equipment seemed to need it or not, so we only have the 125 flights off of each engine. We have data from 100 engines, each stored in a spearate file. We'll start by looking at just one data file for engine 1.

```{r Read in data}

column_names <- c(
  'Unit', 
  'Time', 
  'Setting1', 
  'Setting2', 
  'Setting3', 
  'FanInletTemp',
  'LPCOutletTemp', 
  'HPCOutletTemp', 
  'LPTOutletTemp', 
  'FanInletPres', 
  'BypassDuctPres', 
  'TotalHPCOutletPres', 
  'PhysFanSpeed', 
  'PhysCoreSpeed', 
  'EnginePresRatio', 
  'StaticHPCOutletPres', 
  'FuelFlowRatio', 
  'CorrFanSpeed', 
  'CorrCoreSpeed', 
  'BypassRatio', 
  'BurnerFuelAirRatio', 
  'BleedEnthalpy', 
  'DemandFanSpeed', 
  'DemandCorrFanSpeed', 
  'HPTCoolantBleed', 
  'LPTCoolantBleed'
)

sensor_data <- 
  read.table("CMAPSSData/train_FD001.txt", 
             col.names = column_names)

engine01 <- sensor_data[sensor_data$Unit == 1, ]
engine01 <- engine01[1:125, ]

head(engine01)
```

# View subset of sensor signals

We have a total of 21 sensors for each engine. Since that's a lot to put on the screen at once, let's look at just the first nine sensors. We can see that some of them are flat, so they won't be useful for understanding how the condition of the system is changing. Others need a bit of smoothin to remove some noise. The other 12 sensors we are not looking at here behave similarly.

```{r View subset (engine 1)}

view_columns <- c(
  'TotalHPCOutletPres',
  'PhysFanSpeed',
  'PhysCoreSpeed',
  'StaticHPCOutletPres',
  'FuelFlowRatio',
  'CorrFanSpeed',
  'CorrCoreSpeed',
  'BypassRatio',
  'BleedEnthalpy'
)

par(mfrow = c(3, 3))
for(ii in 1:9) {
  plot(x = engine01$Time, 
       y = engine01[, 5+ii], 
       type = "l",
       col = "deepskyblue3",
       xlim = c(0, 125),
       xaxs = "i",
       main = colnames(engine01)[5+ii],
       xlab = "Time",
       ylab = "")
}
```

# Select relevant variable names based on visualization

Here we select the variables we want to keep, getting rid of the sensors that were flat and unuseful.

```{r Select relevant variable names (engine 1)}
variable_names <- c(
  'Unit',
  'Time',
  'LPCOutletTemp', 
  'HPCOutletTemp', 
  'LPTOutletTemp', 
  'TotalHPCOutletPres', 
  'PhysFanSpeed', 
  'PhysCoreSpeed',
  'StaticHPCOutletPres', 
  'FuelFlowRatio', 
  'CorrFanSpeed', 
  'CorrCoreSpeed', 
  'BypassRatio', 
  'BleedEnthalpy', 
  'HPTCoolantBleed', 
  'LPTCoolantBleed'
)
engine01 = engine01[, variable_names]
```

# Remove noise

There is some amount of noise in the signal. There are many advanced methods in MATLAB to identify and remove noise. Here we simply use a trailing moving average t osmooth the signals slightly.

```{r Remove noise (engine 1)}
k <- 4
engine01_smooth <- engine01

## Implementation of moving average with filling truncated areas
## First value remains the same
## Second to kth value is truncated, and averaged over values < k
for (i in 2:k) {
  engine01_smooth[i, 3:ncol(engine01_smooth)] <- 
    apply(engine01[1:i, 3:ncol(engine01)], 2, mean)
}

for(i in (k+1):nrow(engine01_smooth)) {
  engine01_smooth[i, 3:ncol(engine01_smooth)] <- 
    apply(engine01[(i-k):i, 3:ncol(engine01)], 2, mean)
}

## Remove truncated areas
engine01_smooth <- engine01_smooth[(k+2):nrow(engine01),]
head(engine01_smooth, 20)
```

# Plot smoothed data

We are down to 14 sensors from 21 after removing the sensors that were constant signals. After smoothing the remaining signals, we can now see the signals we have to work with.

```{r Plot smoothed data (engine 1)}
par(mfrow = c(3, 3))
for(colname in view_columns) {
  plot(x = engine01_smooth$Time, 
       y = engine01_smooth[, colname], 
       type = "l",
       col = "deepskyblue3",
       xlim = c(0, 125),
       #ylim = c(552, 556),
       xaxs = "i",
       main = colname,
       xlab = "Time",
       ylab = "")
}
```


# Monitoring equipment - control charts

How can we use these signals to determine if the equipment is in normal conditions? One comming method is using what is called a control cart. As long as our signal stays within the upper and lower control limits, we might consdiger it normal conditions. If the signal goes outside the limits, then we might have a problem.

However when we have a large number of signals, such as the 14 we have here, it is difficult to determine when we might have a problem. Is one sensor going outside the bounds for 1 point a problem? 5 sensors for 3 points? 10 sensors for 20 points? control charts become difficult to use in these cases, so we will bring in machine learning to help us.

```{r Control chart}
## Using correction factor 1.3 when computing standard error
plot(engine01$Time, engine01$LPCOutletTemp,
     ylim = c(641.5, 643.5),
     main = 'Control chart',
     xlab = 'Time',
     ylab = 'LPTOutletTemp',
     pch = 16,
     col = "blue"
)
lines(engine01$Time, engine01$LPCOutletTemp, col = "blue")
mean_temp <- mean(engine01$LPCOutletTemp)
stderror <- sd(engine01$LPCOutletTemp)/sqrt(1.3)
ucl <- mean_temp + stderror * 3
lcl <- mean_temp - stderror * 3
abline(mean_temp, 0, col = 'green')
abline(ucl, 0, col = 'red')
abline(lcl, 0, col = 'red')
points(engine01$Time[engine01$LPCOutletTemp > ucl], 
       engine01$LPCOutletTemp[engine01$LPCOutletTemp > ucl], 
       col = "red")
```

# Read all data

Before we try using machine learning, let's read in all the data we have available. Our data files are strored in a single folder, with each of the hundred engines stored in a separate file. We will use a datastore to easily read them all in.

```{r Read (and smooth) all data}
## All units already available in sensor_data
k <- 4

sensor_data_smooth <- c()
for (u in unique(sensor_data$Unit)) {
  ## Prepare data
  data_temp <- sensor_data[sensor_data$Unit == u, variable_names]
  data_temp <- data_temp[1:125,]

  data_temp_smooth <- data_temp
  ## Implementation of moving average with filling truncated areas
  ## First value remains the same
  ## Second to kth value is truncated, and averaged over values < k
  for (i in 2:k) {
    data_temp_smooth[i, 3:ncol(data_temp_smooth)] <- 
      apply(data_temp[1:i, 3:ncol(data_temp)], 2, mean)
  }

  for(i in (k+1):nrow(data_temp_smooth)) {
    data_temp_smooth[i, 3:ncol(data_temp_smooth)] <- 
      apply(data_temp[(i-k):i, 3:ncol(data_temp)], 2, mean)
  }
  
  ## remove truncated areas
  data_temp_smooth <- data_temp_smooth[(k+2):nrow(data_temp_smooth), ]
  sensor_data_smooth <- rbind(sensor_data_smooth, data_temp_smooth)
}

head(sensor_data_smooth, 10)

```

# Plot all data

Now we can visualize all 100 eninges plotted on top of each other. We can now see that there is not a clear signal over time, but rather a range of values that the sensors could take.

```{r Plot all data}
par(mfrow = c(3, 3))
for (colname in view_columns) {
  plot(x = sensor_data_smooth[sensor_data_smooth$Unit == 1, "Time"], 
       y = sensor_data_smooth[sensor_data_smooth$Unit == 1, colname], 
       type = "l",
       xaxs = "i",
       col = "deepskyblue3",
       xlim = c(0, 125),
       ylim = c(min(sensor_data_smooth[, colname], na.rm = T),
                max(sensor_data_smooth[, colname], na.rm = T)),
       main = colname,
       xlab = "Time",
       ylab = "")
  
  for (u in unique(sensor_data_smooth$Unit)) {
    lines(x = sensor_data_smooth[sensor_data_smooth$Unit == u, "Time"], 
          y = sensor_data_smooth[sensor_data_smooth$Unit == u, colname],
          col = "deepskyblue3")
  }
}
```

# Standardize data

Many machine learning techniques, such as PCA involve measuring distances between points. However machine learning algorithms do not understand concepts like units. Since the data is recorded with different scales and units we need to standardize the data in some way. There are many different ways we might standardize data based on its propoerties and our analysis goals. Just as one example, here we give each signal the same mean of zero and standard deviation of one.

```{r Standardize data}
## Pull out just the sensor data, ignoring the unit and timestamp
xtrain <- sensor_data_smooth[, 3:ncol(sensor_data_smooth)]

## Give all sensors mean of zero and standard deviation of one
# xtrain_mean <- apply(xtrain, 2, mean)
# xtrain_sd <- apply(xtrain, 2, sd)
# xtrain_standard <- (xtrain - xtrain_mean) / xtrain_sd 
xtrain_standard <- apply(xtrain, 2, function(x) 
 scale(x, center = T, scale = T))
#xtrain_standard <- scale(xtrain, center = T, scale = T)
# xtrain_standard <- apply(xtrain, 2, function(x)
#   (x - mean(x) ) / (sd(x) / length(x) ))
```

# Dimensionality reduction - PCA

Principal component analysis (PCA) is one of the most popular methods of reducing the dimensionality of the data by rotating our axes such that they point in the direction of maximum variance. This allows us to summarize a large proportion of the data in a smaller dimensional data set. In this case, note that the first two principal component capture a good portion of the variance in the data set but further principal components only capture a small amount of additional information.

```{r Dimensionality reduction}
## Apply principal components analysis to the standardized data
## MATLAB's pca uses singular value decomposition (SVD)
## ... so R prcomp is appropriate (vs. princomp, which uses spectral decomposition)
pca <- prcomp(xtrain_standard)
pca_summary <- summary(pca)

## prcomp(xtrain_standard)[1] == princomp(xtrain_standard, cor = T)[1]

## Plot the total variance explained by each individual principal component
## ... along with the cumulative total that has been explained
plot(pca_summary$importance[3, 1:10] * 100,
# plot(cumsum(pca_summary$importance[2, 1:10])/sum(pca_summary$importance[2, ]) * 100,
     pch = 16, col = 'blue',
     ylim = c(0, 100),
     main = 'Individual and cumulative variance explained by PCA',
     xlab = "# of principal component",
     ylab = '% of variance of dataset explained')
axis(side = 1, at = 1:10)
axis(side = 2, at = seq(0, 100, 10))
points(pca_summary$importance[2, 1:10] * 100, pch = 16, col = 'red')
# points(pca_summary$importance[2, 1:10]/sum(pca_summary$importance[2, ]) * 100, pch = 16, col = 'red')

## Visualize the first 2 principal components
## ... which explain nearly 90% of the overall variance
plot(pca$x[,1], pca$x[,2],
     pch = 16,
     cex = 0.2,
     col = "deepskyblue3",
     xlab = "First principal component",
     ylab = "Second principal component")
     #xlim = c(-10, 15))

## Save the needed variables to standardize new data 
## ...and transform it into the same PCA projection
```

# Warning system

Now we need to find some way to use this group of points to determine when conditions are deviating away from 'normal'. A comming approach is to call the region that contains densely packed points 'normal' conditions, and the small number of points that fall outside of them to be 'anormal' and potentially in need of maintenance. We need some way to determine when an engine might transition from 'normal' to 'abnormal' behavior.

 # Visualize first and last points for each engine

As a first try we can simply look at the first and last point recorded for each engine. If engines tend to start in a certain area of the PCA, but move to a different area by the time maintenance is performed, this may give us some indiciatoin of what the trens towards failure looks like.

In this case we can see that the first and last points form two groups, with the first points centered closer to the origin and the last points centered further away. However there is significant overlap between them, which would not be suprising if many engines are still behaving normally at the time maintenance was performed.

```{r Warning system}

```

# Highlight problematic engines

The prio plot showed much overlap betwen the first and last data points. However some of the last data points clearly fall well outside the main cluster. Our maintenance staff identified several engines that had showed signs of degredation. Here we can show the points just from these engines as they approach their maintenance date to see if they show any clear trends. They are plotted on top of all other points to see how they may be different.

We can see that many of them do indeed fall outside the main cluster of points, indicating that if an engine's sensor readings move outside the main cluster it may make for a dood cirteria for when an engine needs servicing.

```{r Highlight problematic engines}

```

# Path through PCA

Fur further investigation we can see how different engines move through the cluster of data poings as they approach their maintenance date.

```{r Path through PCA}

```

# Inertial criteria for warning and alarm signals

Based on the results, we can make an initial guess for the criteria to use for when to issue a 'warning' or 'alarm' signal. Clearly all engines begin within the main cluster of points. It seems that they may move outside the cluster as servicing becomes more needed. We will generate a 'warn' result' when the engine behavior has left 'normal' conditions (in this case 'normal' includes about 90% of the data) and should be prioritized for maintenance. An 'alarm' means that this engine is very far from 'normal' and may need maintenance urgently. Below is one example of an initial guess we may make for where to draw the line between the 'normal', 'warn' and 'alarm' regions.

The criteria for producing the 'warn' and 'alarm' signals should be treated as initial guesses that will continue to evolve over time as we perform more rounds of maintenance while gathering more sensor data and maintenance feedback.

```{r Inertial criteria}

```

# How can evaluate the criteria?

Since we have never had an equiment failure, we have no way of knowing what failure looks like in the data. Even though our maintenance staff identified several engines that appeared to be degrading, and those engines appeared to show a distinct trend closer to the maintenance date, we sill don't know how close to failure they actually were. So then how can we use this information to improve our decision making? This is one of the most difficualt aspects of unsupervised learning.

There are a number of strategies for how to use these results. Our conservative strategy is to slowly roll back the regularly scheduled maintenance dates. For example, this data came from our first round of maintenance. On the next maintenance round, we might wait to perform maintenance unteil equipment cycle 135 instead of 125, unless a piece of equipment signals an 'alarm'. If it does, we could priotirize maintenance for that pience of equipment. Otherwise, we will wait longer that the last time to do scheduled maintenance. Along the way we can use the additional sensor data and feedback from maintenance to update our 'warn' and 'alarm' criteria.

We can continue to roll back the pace of our regularly scheduled maintenance dates, unteil we are scheduling our maintenance entirely on when we get the 'warn' and 'alarm' signals from our equipment. If a failure does occur in spite of our effors, SAVE THAT DATA! Real world data leading up to and including failure is both very valuable and very expensive to gather. The better we can understand what failure looks like in the data, the better our results can get.

# Evaluate all engines to failure

We actually do have data from all 100 engines running and failure conditions area reached, we were just ignoring what happened after sample 125 so we could explore an unsupervised workflow in a scheduled maintenance situation. Here we load in all the data from all 100 engines to dtermine how this would turned out if we run all our engines to failure while tracking their warning and alarm signals.

We can see how has the engines appraoch failure the ratio of 'normal', 'warning' and 'alarm' classifications change over time. All engines are 'normal' unteil aounrd 125 flights prior to failure, at which point some begin to enter the 'warning' region. Around 75 flights prior to failure, some begin to enter the 'alarm' region- All engines have left 'normal' conditions by 26 flights prior to failure. By 9 flights prio to failure, all 100 engines are triggering alarms, so no engine enters failure without triggering an alarm for a least 9 consecutive flights.


```{r Evaluate all engines}

```

# How much uptime did we gain?

If we were to use this method in place of our regularly scheduled maintenance, how many additional cycles of operation would we gain? Here we look at how many cycles we would gain if we either performed maintenence as soon as the warn signal is generated, or waited and performend maintenance once an alarm is triggerd

This data was published by NASA and they curated it to start failry close to failre, so some of these percentages my be high compared to other applications. However the gerenal outcome tends to be the same. usaually there is a consecuative strategy (such as maintenance on warning) that may give us a small percentage boost over scheduled maintenance, a more aggressive strategy (maintenance on alarm) that will give us more, and then a maximum amount we could gain if we did maintenance one sample prior to failure.

```{r Uptime}

```

